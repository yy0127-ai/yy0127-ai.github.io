<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>6.4多输入多输出通道</title>
    <url>/2023/11/07/6-4%E5%A4%9A%E8%BE%93%E5%85%A5%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93/</url>
    <content><![CDATA[<p><strong>多通道，当添加通道时，输入和隐藏的表示都变成三维张量。例如，每个RGB输入图像具有3 x h x w的形状，将这个大小为3的轴称为通道维度</strong></p>
<blockquote>
<p>当通道&gt;1时，卷积核的每个输入通道将包含一个二维张量，由于输入和卷积核都有c个通道，可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和得到二维张量。</p>
</blockquote>
<p><img src="../images/1.png"></p>
<p><strong>实现多输入通道互相关运算，实际上是对每个通道执行互相关运算操作后将结果相加</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 先遍历X和K的第0个维度(通道维度)，再把它们加在一起</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">               [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]]) <span class="comment"># 2 * 3 * 3</span></span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]]) <span class="comment"># 2 * 2 * 2</span></span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"><span class="comment"># tensor([[ 56.,  72.],</span></span><br><span class="line"><span class="comment">#       [104., 120.]])</span></span><br></pre></td></tr></table></figure>

<h3 id="2-多输出通道"><a href="#2-多输出通道" class="headerlink" title="2.多输出通道"></a>2.多输出通道</h3><p><strong>实现一个计算多个通道的输出的互相关函数</strong></p>
<blockquote>
<p>用Ci和Co分别表示输入和输出通道的数目，并让Kℎ和Kw为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为Ci×Kℎ×Kw的卷积核张量，这样卷积核的形状是Co×Ci×Cℎ×Cw。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 迭代K的第0个维度，每次都对输入X执行互相关运算</span></span><br><span class="line">    <span class="comment"># 最后将所有结果都叠加在一起</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 通过将核张量K与K+1(K种种每个元素+1) 和K+2连接起来，构造了一个具有3个输出通道的卷积核</span></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape  <span class="comment"># 3*2*2*2</span></span><br><span class="line"><span class="comment"># 下面，我们对输入张量X与卷积核张量K执行互相关运算。现在的输出包含</span></span><br><span class="line"><span class="comment"># 个通道，第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致</span></span><br><span class="line">corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="comment"># 结果：tensor([[[ 56.,  72.],</span></span><br><span class="line"><span class="comment">#        [104., 120.]],</span></span><br><span class="line"><span class="comment">#       [[ 76., 100.],</span></span><br><span class="line"><span class="comment">#        [148., 172.]],</span></span><br><span class="line"><span class="comment">#        [[ 96., 128.],</span></span><br><span class="line"><span class="comment">#        [192., 224.]]])</span></span><br></pre></td></tr></table></figure>

<h3 id="3-1-1卷积层"><a href="#3-1-1卷积层" class="headerlink" title="3.   1 * 1卷积层"></a>3.   1 * 1卷积层</h3><blockquote>
<p>使用1×1卷积核与3个输入通道和2个输出通道的互相关计算。 这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。 我们可以将1×1卷积层看作在每个像素位置应用的全连接层，以Ci个输入值转换为Co个输出值。 因为这仍然是一个卷积层，所以跨像素的权重是一致的。 同时，1×1卷积层需要的权重维度为Co×Ci，再额外加上一个偏置。</p>
</blockquote>
<p><img src="../images/2.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用全连接层实现1 * 1卷积，需要对输入和输出的数据形状进行调整</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape  <span class="comment"># 输入通道数 高度 宽度</span></span><br><span class="line">    c_o = K.shape[<span class="number">0</span>] <span class="comment"># 输出通道数</span></span><br><span class="line">    X = X.reshape((c_i, h * w))</span><br><span class="line">    K = K.reshape((c_o, c_i))</span><br><span class="line">    <span class="comment"># 全连接中的矩阵乘法</span></span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))</span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>6.3填充与步幅</title>
    <url>/2023/11/07/6-3%E5%A1%AB%E5%85%85%E4%B8%8E%E6%AD%A5%E5%B9%85/</url>
    <content><![CDATA[<p><strong>影响卷积输出的大小：卷积核的大小、填充和步幅</strong></p>
<h3 id="1-填充"><a href="#1-填充" class="headerlink" title="1.填充"></a>1.填充</h3><blockquote>
<p>在应用多层卷积时，常常丢失边缘像素。由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。 但随着我们应用许多连续卷积层，累积丢失的像素数就多了。 解决这个问题的简单方法即为<em>填充</em>（padding）：在输入图像的边界填充元素（通常填充元素是0）。</p>
<p>卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。</p>
<p>此外，使用奇数的核大小和填充大小也提供了书写上的便利。对于任何二维张量<code>X</code>，当满足： 1. 卷积核的大小是奇数； 2. 所有边的填充行数和列数相同； 3. 输出与输入具有相同高度和宽度 则可以得出：输出<code>Y[i, j]</code>是通过以输入<code>X[i, j]</code>为中心，与卷积核进行互相关计算得到的。</p>
</blockquote>
<p><strong>在下面的例子中，我们创建一个高度和宽度为3的二维卷积层，并在所有侧边填充1个像素。给定高度和宽度为8的输入，则输出的高度和宽度也是8。</strong></p>
<blockquote>
<p>因为padding=1，则8+2=10，输入为10 x 10，K = 3 x 3，则输出为(10-3+1) x (10-3+1) = 8 x 8</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment"># 定义一个计算卷积层的函数</span></span><br><span class="line"><span class="comment"># 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># 这里的(1, 1)表示批量大小和通道数都为1</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)  <span class="comment"># X.shape  (1, 1, 8, 8)</span></span><br><span class="line">    Y = conv2d(X)  <span class="comment"># 二维卷积层使用四维输入和输出格式</span></span><br><span class="line">    <span class="comment"># 省略前两个维度：批量大小和通道</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"><span class="comment"># 每边都填充1行或1列，因此总共添加2行或2列</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="comment"># 结果：torch.Size([8, 8])</span></span><br></pre></td></tr></table></figure>

<p><strong>当卷积核的高度和宽度不同时，我们可以填充不同的高度和宽度，使输出和输入具有相同的高度和宽度。在如下示例中，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>,  <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="comment"># 结果：torch.Size([8, 8])</span></span><br></pre></td></tr></table></figure>

<h3 id="2-步幅"><a href="#2-步幅" class="headerlink" title="2.步幅"></a>2.步幅</h3><blockquote>
<p>在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素</p>
<p>当垂直步幅为Sh、水平步幅为Sw时，输出形状为：(Nh-Kh+Ph+Sh)/Sh向下取整 + (Nw-Kw+Pw+Sw)/Sw向下取整。</p>
<p>如果我们设置了Pℎ=Kℎ−1和Pw=Kw−1，则输出形状将简化为⌊(Nℎ+Sℎ−1)/Sℎ⌋×⌊(Nw+Sw−1)/Sw⌋。 更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为(Nℎ/Sℎ)×(Nw/Sw)。</p>
</blockquote>
<p><strong>将高度和宽度的步幅设置为2，从而将输入的宽度和高度减半</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="comment"># torch.Size([4, 4])</span></span><br></pre></td></tr></table></figure>

<p><strong>复杂一点的例子</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"><span class="comment"># torch.Size([2, 2])</span></span><br></pre></td></tr></table></figure>

<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h3><ul>
<li>填充可以增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽。</li>
<li>步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的1/n（n是一个大于1的整数）。</li>
<li>填充和步幅可用于有效地调整数据的维度。</li>
</ul>
]]></content>
      <categories>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/11/06/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>leetcode刷题---题解</title>
    <url>/2023/11/07/leetcode%E5%88%B7%E9%A2%98-%E9%A2%98%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="Leetcode题解-排序"><a href="#Leetcode题解-排序" class="headerlink" title="Leetcode题解-排序"></a>Leetcode题解-排序</h3><h4 id="215-数组中的第k个最大元素"><a href="#215-数组中的第k个最大元素" class="headerlink" title="215.数组中的第k个最大元素"></a>215.数组中的第k个最大元素</h4><blockquote>
<p>给定整数数组 nums 和整数 k，请返回数组中第 k 个最大的元素。<br>请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。<br>你必须设计并实现时间复杂度为 O(n) 的算法解决此问题。</p>
</blockquote>
<p><strong>示例1：</strong></p>
<blockquote>
<p>输入：[3, 2, 1, 5, 6,4] ，k=2<br>输出: 5</p>
</blockquote>
<p><strong>自己想法：首先排序，最后取出第k大的元素(然后发现自己不会写快排)</strong><br><strong>解法一：暴力解法</strong></p>
<blockquote>
<p>分析一下：找到第k个最大的元素，而不是第k个不同元素，意味着升序排完序后，从数组右侧数的第k个元素，则从左边数为len-k</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">findKthLargest</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> k)</span> &#123;</span><br><span class="line">        <span class="comment">//暴力解法  用数组的排序方法进行求解</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> nums.length;</span><br><span class="line">        Arrays.sort(nums);</span><br><span class="line">        <span class="keyword">return</span> nums[len - k];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>复杂度分析：</strong></p>
<ul>
<li>时间复杂度：O(NlogN)，JDK默认使用快速排序；</li>
<li>空间复杂度：O(logN)，空间复杂度递归调用栈的高度；</li>
</ul>
<p><strong>解法二：减而治之—-快速选择算法</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>(System.currentTimeMillis());</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">findKthLargest</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> k)</span> &#123;</span><br><span class="line">        <span class="comment">//快速排序</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> nums.length;</span><br><span class="line">        <span class="type">int</span> <span class="variable">target</span> <span class="operator">=</span> len - k;</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> <span class="number">0</span>; </span><br><span class="line">        <span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> len-<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">pivotIndex</span> <span class="operator">=</span> partition(nums, left, right);</span><br><span class="line">            <span class="keyword">if</span>(target == pivotIndex)&#123;</span><br><span class="line">                <span class="keyword">return</span> nums[pivotIndex];</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(pivotIndex &gt; target)&#123;</span><br><span class="line">                right = pivotIndex - <span class="number">1</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                left = pivotIndex + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> left, <span class="type">int</span> right)</span>&#123;</span><br><span class="line">        <span class="comment">//进行排序、交换</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">randomIndex</span> <span class="operator">=</span> left + random.nextInt(right-left + <span class="number">1</span>);</span><br><span class="line">        swap(nums, left, randomIndex);</span><br><span class="line">        <span class="type">int</span> <span class="variable">pivot</span> <span class="operator">=</span> nums[left]; <span class="comment">//取一个基值</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">le</span> <span class="operator">=</span> left + <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">ge</span> <span class="operator">=</span> right;</span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            <span class="keyword">while</span>(le &lt;= ge &amp;&amp; nums[le] &lt; pivot)&#123;  <span class="comment">//发现里面的判断条件是有先后关系的，最先跳出while的是le&lt;=ge如果不满足这个条件不可能nums[le] &lt; pivot</span></span><br><span class="line">                le++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span>(le &lt;= ge &amp;&amp; nums[ge] &gt; pivot)&#123;</span><br><span class="line">                ge--;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(le &gt;= ge)&#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            swap(nums, le, ge);</span><br><span class="line">            le++;</span><br><span class="line">            ge--;</span><br><span class="line">        &#125;</span><br><span class="line">        swap(nums, left, ge);</span><br><span class="line">        <span class="keyword">return</span> ge;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">swap</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> index1, <span class="type">int</span> index2)</span>&#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">tmp</span> <span class="operator">=</span> nums[index1];</span><br><span class="line">        nums[index1] = nums[index2];</span><br><span class="line">        nums[index2] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>复杂度分析：</strong></p>
<ul>
<li>时间复杂度：O(N)</li>
<li>空间复杂度：O(1)  (没有使用递归)</li>
<li><em>解法三：优先队列</em>*<blockquote>
<p>找到第K大元素，也就是整个数组排序后后半部分最小的那个元素。所以可以用k个元素的最小堆</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">findKthLargest</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> k)</span> &#123;</span><br><span class="line">       PriorityQueue&lt;Integer&gt; heap = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;&gt;();   <span class="comment">//定义一个优先队列</span></span><br><span class="line">       <span class="keyword">for</span>(<span class="type">int</span> num : nums)&#123;  <span class="comment">//队列中只存k个元素，对顶就是k个元素中最小的那个，也就是第k大的元素</span></span><br><span class="line">           heap.add(num);  <span class="comment">//内部：新插入的元素 与其父节点进行比较，然后调整堆</span></span><br><span class="line">           <span class="keyword">if</span>(heap.size() &gt; k)&#123;  <span class="comment">//元素个数超过k</span></span><br><span class="line">               heap.poll();  <span class="comment">//移除头部元素 最后一个元素被移动到堆顶，在进行堆调整</span></span><br><span class="line">           &#125;</span><br><span class="line">       &#125; </span><br><span class="line">       <span class="keyword">return</span> heap.peek();  <span class="comment">//返回堆顶元素</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>leetcode刷题</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>6.2卷积神经网络—图像卷积</title>
    <url>/2023/11/07/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF/</url>
    <content><![CDATA[<h3 id="1-互相关运算"><a href="#1-互相关运算" class="headerlink" title="1.互相关运算"></a>1.互相关运算</h3><h4 id="1-1在corr2d函数中实现卷积过程"><a href="#1-1在corr2d函数中实现卷积过程" class="headerlink" title="1.1在corr2d函数中实现卷积过程"></a>1.1在corr2d函数中实现卷积过程</h4><p><strong>该函数接受输入张量X和卷积核张量K，并返回输出张量Y</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape  <span class="comment"># 卷积核的高 宽</span></span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))  <span class="comment"># 初始化Y</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i+h, j:j+w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>

<p><strong>输入张量X和卷积核张量K，验证上述二维互相关运算的输入</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br></pre></td></tr></table></figure>

<p><strong>输出结果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[19., 25.],</span><br><span class="line">        [37., 43.]])</span><br></pre></td></tr></table></figure>

<h3 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2.卷积层"></a>2.卷积层</h3><p><strong>卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。因此卷积层中两个被训练的参数为卷积核权重和标量偏置</strong></p>
<p><strong>基于上面定义的corr2d函数实现二维卷积层。在__init__构造函数中，将<code>weight</code>和<code>bias</code>声明为两个模型参数，前向传播函数调用<code>corr2d</code>函数并添加偏置</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 卷积层</span></span><br><span class="line"><span class="comment"># 随机初始化权重</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + bias</span><br></pre></td></tr></table></figure>

<p><strong>使用上述函数：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = Conv2D((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">x = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>],</span><br><span class="line">                   [<span class="number">5.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>],</span><br><span class="line">                   [<span class="number">9.0</span>, <span class="number">10.0</span>, <span class="number">11.0</span>, <span class="number">12.0</span>],</span><br><span class="line">                   [<span class="number">13.0</span>, <span class="number">14.0</span>, <span class="number">15.0</span>, <span class="number">16.0</span>]], dtype=torch.float32)</span><br><span class="line">output = net(x)</span><br><span class="line">output</span><br></pre></td></tr></table></figure>

<p><strong>结果为：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[11.7987, 14.6210, 17.4432],</span><br><span class="line">        [23.0878, 25.9100, 28.7323],</span><br><span class="line">        [34.3768, 37.1990, 40.0213]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="3-图像中目标的边缘检测"><a href="#3-图像中目标的边缘检测" class="headerlink" title="3.图像中目标的边缘检测"></a>3.图像中目标的边缘检测</h3><p><strong>通过找到像素变化的位置，来检测图像中不同颜色的边缘。 首先，我们构造一个6×8像素的黑白图像。中间四列为黑色（0），其余像素为白色（1）。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones((<span class="number">6</span>,<span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span><br></pre></td></tr></table></figure>

<p><strong>接下来，我们构造一个高度为1、宽度为2的卷积核<code>K</code>。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br></pre></td></tr></table></figure>

<p><strong>现在，我们对参数<code>X</code>（输入）和<code>K</code>（卷积核）执行互相关运算。 如下所示，输出<code>Y</code>中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br><span class="line">Y</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</span><br></pre></td></tr></table></figure>

<p><strong>现在我们将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核<code>K</code>只可以检测垂直边缘，无法检测水平边缘。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corr2d(X.t(), K)</span><br><span class="line"><span class="comment">#tensor([[0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure>

<h3 id="4-学习卷积核"><a href="#4-学习卷积核" class="headerlink" title="4.学习卷积核"></a>4.学习卷积核</h3><p><strong>如果我们只需寻找黑白边缘，那么以上<code>[1, -1]</code>的边缘检测器足以。然而，当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。那么我们是否可以学习由<code>X</code>生成<code>Y</code>的卷积核呢？</strong></p>
<p><strong>现在让我们看看是否可以通过仅查看“输入-输出”对来学习由<code>X</code>生成<code>Y</code>的卷积核。 我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较<code>Y</code>与卷积层输出的平方误差，然后计算梯度来更新卷积核。为了简单起见，我们在此使用内置的二维卷积层，并忽略偏置。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 学习卷积核</span></span><br><span class="line"><span class="comment"># 构造一个二维卷积层，具有1个输出通道和形状为(1, 2)的卷积核</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>) <span class="comment"># 1 1 表示输入通道、输出通道</span></span><br><span class="line"><span class="comment"># 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度）</span></span><br><span class="line"><span class="comment"># 其中批量大小和通道数都为1</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line">lr = <span class="number">3e-2</span> <span class="comment"># 学习率</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span> <span class="comment"># 计算损失</span></span><br><span class="line">    conv2d.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">    l.<span class="built_in">sum</span>().backward()  <span class="comment"># 向后传播</span></span><br><span class="line">    <span class="comment"># 迭代卷积</span></span><br><span class="line">    conv2d.weight.data[:] -= lr * conv2d.weight.grad  <span class="comment"># 更新梯度</span></span><br><span class="line">    <span class="keyword">if</span>(i + <span class="number">1</span>) %<span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>结果为：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">epoch 2, loss 2.360</span><br><span class="line">epoch 4, loss 0.514</span><br><span class="line">epoch 6, loss 0.134</span><br><span class="line">epoch 8, loss 0.042</span><br><span class="line">epoch 10, loss 0.015</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># tensor([[ 0.9590, -1.0091]]) 卷积核权重的结果很接近之前定义的卷积核K</span></span><br></pre></td></tr></table></figure>

<h3 id="5-小结"><a href="#5-小结" class="headerlink" title="5.小结"></a>5.小结</h3><ul>
<li>二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。</li>
<li>我们可以设计一个卷积核来检测图像的边缘。</li>
<li>我们可以从数据中学习卷积核的参数。</li>
<li>学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。</li>
<li>当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。</li>
</ul>
]]></content>
      <categories>
        <category>卷积神经网络</category>
      </categories>
      <tags>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>myfirst_blog</title>
    <url>/2023/11/06/myfirst-blog/</url>
    <content><![CDATA[<h3 id="1-安装Git"><a href="#1-安装Git" class="headerlink" title="1.安装Git"></a>1.安装Git</h3><h3 id="2-安装nodejs"><a href="#2-安装nodejs" class="headerlink" title="2.安装nodejs"></a>2.安装nodejs</h3><span id="more"></span>

<h3 id="3-安装hexo"><a href="#3-安装hexo" class="headerlink" title="3.安装hexo"></a>3.安装hexo</h3><h3 id="4-编写博客"><a href="#4-编写博客" class="headerlink" title="4.编写博客"></a>4.编写博客</h3><ol>
<li><p>新建文章</p>
<ol>
<li>```<br>hexo new newpapername<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">2. 在source/_post中打开markdown文档，开始编辑，写完保存，使用下面命令</span><br><span class="line"></span><br><span class="line">   1. ```</span><br><span class="line">      hexo clean</span><br><span class="line">      hexo g</span><br><span class="line">      hexo d  # 部署</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
]]></content>
      <tags>
        <tag>新手</tag>
        <tag>start</tag>
      </tags>
  </entry>
</search>
